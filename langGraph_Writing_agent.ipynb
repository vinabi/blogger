{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langchain langchain-openai pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yrbm24_mTKBy",
        "outputId": "0534d3ed-2824-40ce-c564-c4d948faf25f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.75)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.6-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.104.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m391.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Downloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.6-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph\n",
            "Successfully installed langchain-openai-0.3.32 langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.6 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "66879322",
        "outputId": "11111067-08b5-4ef7-9326-b78368deb794"
      },
      "source": [
        "!pip install -q langchain-groq"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/134.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/134.9 kB\u001b[0m \u001b[31m843.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m122.9/134.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\") or os.getenv(\"GROQ_API_KEY\")\n",
        "except Exception:\n",
        "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.4,\n",
        ")"
      ],
      "metadata": {
        "id": "04iW_0DYVO-1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal, List, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.tools import tool\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "_odk5mbfUoWF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 0) Real-time user input (Notebook/Console)\n",
        "# -----------------------------\n",
        "def get_user_profile():\n",
        "    print(\"▶ Blog Generator — provide a few details.\")\n",
        "    topic = input(\"Topic: \").strip()\n",
        "    audience = input(\"Audience: \").strip()\n",
        "    tone = input(\"Tone (casual/friendly/professional/technical): \").strip() or \"friendly\"\n",
        "    words = input(\"Target word count (e.g., 900): \").strip()\n",
        "    words = int(words) if words.isdigit() else 900\n",
        "    notes = input(\"Any extra instructions? \").strip()\n",
        "    return dict(topic=topic, audience=audience, tone=tone, target_words=words, instructions=notes)\n",
        "\n",
        "def ask_user_approval(suggestion: str) -> bool:\n",
        "    print(\"\\n— SUPERVISOR NOTE —\")\n",
        "    print(suggestion)\n",
        "    ans = input(\"Approve and finalize? (y/n): \").strip().lower()\n",
        "    return ans.startswith(\"y\")"
      ],
      "metadata": {
        "id": "5vXWsV4Id5no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2) Tools as functions (callable by LLM)\n",
        "# -----------------------------\n",
        "@tool\n",
        "def keyword_suggest(topic: str) -> List[str]:\n",
        "    \"\"\"Suggest 6-10 SEO-friendly keywords for the given blog topic.\"\"\"\n",
        "    base = [\"RAG\", \"retrieval augmented generation\", \"vector database\", \"semantic search\",\n",
        "            \"LLM\", \"content strategy\", \"blog workflow\", \"prompting\", \"citations\", \"hallucinations\"]\n",
        "    return base[:10]\n",
        "\n",
        "@tool\n",
        "def quick_facts(topic: str) -> str:\n",
        "    \"\"\"Produce concise background notes and 5–8 facts (no URLs). Mark uncertainty with [verify].\"\"\"\n",
        "    return (f\"- {topic} often uses a pipeline: splitter → embeddings → vector DB → retriever → prompt → LLM.\\n\"\n",
        "            f\"- Benefits: fewer hallucinations, fresher knowledge, traceability.\\n\"\n",
        "            f\"- Watch-outs: chunking strategy, prompt grounding, evals.\\n\"\n",
        "            f\"- Blogging: outline → questions → retrieve → cite quotes where possible [verify].\\n\")\n",
        "\n",
        "TOOLS = [keyword_suggest, quick_facts]"
      ],
      "metadata": {
        "id": "wDu0AV3LVpos"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 3) Shared State\n",
        "# -----------------------------\n",
        "class BlogState(TypedDict, total=False):\n",
        "    # user input\n",
        "    topic: str\n",
        "    audience: str\n",
        "    tone: Literal[\"casual\",\"friendly\",\"professional\",\"technical\"]\n",
        "    target_words: int\n",
        "    instructions: Optional[str]\n",
        "\n",
        "    # artifacts\n",
        "    ideas: str\n",
        "    outline: str\n",
        "    draft: str\n",
        "    supervisor_notes: str\n",
        "    final: str\n",
        "    keywords: List[str]\n",
        "\n",
        "    # control\n",
        "    decision: Literal[\"revise\",\"approve\"]\n"
      ],
      "metadata": {
        "id": "dIVD6_CDV0ZP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4) Nodes\n",
        "# -----------------------------\n",
        "def intake(state: BlogState) -> BlogState:\n",
        "    \"\"\"Collect real-time inputs from the user (first node).\"\"\"\n",
        "    if not all(k in state for k in (\"topic\",\"audience\",\"tone\",\"target_words\")):\n",
        "        user = get_user_profile()\n",
        "        state.update(user)\n",
        "    return state\n",
        "\n",
        "\n",
        "def ideas_researcher(state: BlogState) -> BlogState:\n",
        "    \"\"\"Node 1: Generate ideas & quick research notes (tool-calling allowed).\"\"\"\n",
        "    llm_with_tools = llm.bind_tools([keyword_suggest, quick_facts])\n",
        "    sys = SystemMessage(content=\"You are an idea researcher. Produce concise idea bullets and optional keyword/tool calls.\")\n",
        "    usr = HumanMessage(content=f\"Topic: {state['topic']}\\nAudience: {state['audience']}\\nTone: {state['tone']}\")\n",
        "    msg = llm_with_tools.invoke([sys, usr])\n",
        "    messages = [sys, usr, msg]\n",
        "    # Execute any tool calls\n",
        "    if getattr(msg, \"tool_calls\", None):\n",
        "        tool_results = ToolNode(TOOLS).invoke({\"messages\": messages})\n",
        "        messages += tool_results[\"messages\"]\n",
        "        messages.append(HumanMessage(content=\"Summarize insights and list keywords from tool outputs.\"))\n",
        "        ideas = llm.invoke(messages).content\n",
        "        # collect keywords if returned by keyword_suggest\n",
        "        kws = []\n",
        "        for m in messages:\n",
        "            if getattr(m, \"name\", \"\") == \"keyword_suggest\" and isinstance(m.content, list):\n",
        "                kws = m.content\n",
        "    else:\n",
        "        ideas = msg.content\n",
        "        kws = []\n",
        "    return {**state, \"ideas\": ideas, \"keywords\": kws}\n",
        "\n",
        "def outliner(state: BlogState) -> BlogState:\n",
        "    \"\"\"Node 2: Turn ideas into a structured outline (H2/H3).\"\"\"\n",
        "    sys = SystemMessage(content=\"You are an expert outliner. Use markdown H2/H3; include intro & conclusion.\")\n",
        "    usr = HumanMessage(content=f\"Ideas:\\n{state['ideas']}\\nKeywords: {', '.join(state.get('keywords', []))}\")\n",
        "    outline = llm.invoke([sys, usr]).content\n",
        "    return {**state, \"outline\": outline}\n",
        "\n",
        "def writer(state: BlogState) -> BlogState:\n",
        "    \"\"\"Node 3: Write the draft from outline.\"\"\"\n",
        "    sys = SystemMessage(content=\"You are a blog writer. Produce a cohesive draft with headings, TL;DR, and clear paragraphs.\")\n",
        "    usr = HumanMessage(content=(\n",
        "        f\"Topic: {state['topic']}\\nAudience: {state['audience']}\\nTone: {state['tone']}\\n\"\n",
        "        f\"Target words: {state['target_words']}\\nOutline:\\n{state['outline']}\\n\"\n",
        "        f\"Extra instructions: {state.get('instructions','')}\"\n",
        "    ))\n",
        "    draft = llm.invoke([sys, usr]).content\n",
        "    return {**state, \"draft\": draft}\n",
        "\n",
        "def supervisor(state: BlogState) -> BlogState:\n",
        "    \"\"\"Node 4: Review and decide 'revise' or 'approve'. Also ask the USER for final approval.\"\"\"\n",
        "    sys = SystemMessage(content=(\n",
        "        \"You are a strict supervisor. Give 3–6 concrete improvement notes if needed; else say APPROVED.\"\n",
        "    ))\n",
        "    usr = HumanMessage(content=f\"Evaluate this draft for quality, tone, structure, coherence:\\n\\n{state['draft']}\")\n",
        "    notes = llm.invoke([sys, usr]).content\n",
        "\n",
        "    # If LLM suggests more work OR user wants another pass, go to 'revise'\n",
        "    auto_decision = \"revise\" if \"APPROVED\" not in notes.upper() else \"approve\"\n",
        "    # Ask human in the loop\n",
        "    approved_by_user = ask_user_approval(notes)\n",
        "    decision = \"approve\" if approved_by_user else \"revise\"\n",
        "\n",
        "    # Prefer user’s decision, but we could combine logic if you want\n",
        "    return {**state, \"supervisor_notes\": notes, \"decision\": decision}\n",
        "\n",
        "def finalizer(state: BlogState) -> BlogState:\n",
        "    \"\"\"Node 5: Produce final package (title, meta, tags, slug, final markdown).\"\"\"\n",
        "    sys = SystemMessage(content=(\n",
        "        \"You are a content ops specialist. Produce: Title (<=60 chars), Meta (<=160 chars), \"\n",
        "        \"Slug, 3–5 tags, and Final Markdown body. Keep the user's tone.\"\n",
        "    ))\n",
        "    usr = HumanMessage(content=f\"Draft:\\n{state['draft']}\\nNotes:\\n{state.get('supervisor_notes','')}\\nTone: {state['tone']}\")\n",
        "    final_pkg = llm.invoke([sys, usr]).content\n",
        "    return {**state, \"final\": final_pkg}\n"
      ],
      "metadata": {
        "id": "splKHUgmV6rk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 5) Graph wiring with conditional loop\n",
        "# -----------------------------\n",
        "graph = StateGraph(BlogState)\n",
        "\n",
        "graph.add_node(\"intake\", intake)\n",
        "graph.add_node(\"ideas_researcher\", ideas_researcher)\n",
        "graph.add_node(\"outliner\", outliner)\n",
        "graph.add_node(\"writer\", writer)\n",
        "graph.add_node(\"supervisor\", supervisor)\n",
        "graph.add_node(\"finalizer\", finalizer)\n",
        "\n",
        "graph.set_entry_point(\"intake\")\n",
        "graph.add_edge(\"intake\", \"ideas_researcher\")\n",
        "graph.add_edge(\"ideas_researcher\", \"outliner\")\n",
        "graph.add_edge(\"outliner\", \"writer\")\n",
        "graph.add_edge(\"writer\", \"supervisor\")\n",
        "\n",
        "# conditional edge from supervisor → writer OR finalizer\n",
        "def route_after_supervisor(state: BlogState) -> str:\n",
        "    return \"writer\" if state.get(\"decision\") == \"revise\" else \"finalizer\"\n",
        "\n",
        "graph.add_conditional_edges(\"supervisor\", route_after_supervisor, {\"writer\": \"writer\", \"finalizer\": \"finalizer\"})\n",
        "graph.add_edge(\"finalizer\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "ydU21BYAV-Hv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 6) Run & display final blog\n",
        "# -----------------------------\n",
        "initial = BlogState()  # empty: intake will ask the user in real-time\n",
        "final_state = app.invoke(initial)\n",
        "\n",
        "display(Markdown(\"## Final Blog Output\"))\n",
        "display(Markdown(final_state[\"final\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8Uy89DFzWAhJ",
        "outputId": "3f463b65-0dea-4b76-b7d1-b6c7ee0caed8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶ Blog Generator — provide a few details.\n",
            "Topic: Introduction to RAG evaluation\n",
            "Audience: \n",
            "Tone (casual/friendly/professional/technical): technical\n",
            "Target word count (e.g., 900): 500\n",
            "Any extra instructions? \n",
            "\n",
            "— SUPERVISOR NOTE —\n",
            "APPROVED\n",
            "\n",
            "The draft is well-structured, coherent, and effectively communicates the concept of Retrieval Augmented Generation (RAG) evaluation. Here are some minor suggestions for improvement:\n",
            "\n",
            "1. **Consider adding a section on limitations**: While the draft highlights the benefits of RAG evaluation, it would be beneficial to discuss potential limitations or challenges associated with implementing RAG. This could include limitations in data quality, scalability, or the need for specialized expertise.\n",
            "2. **Use more descriptive headings**: While the headings are clear, they could be more descriptive and engaging. For example, instead of \"Benefits of RAG Evaluation,\" consider \"Unlocking the Power of RAG: Reduced Hallucinations, Fresher Knowledge, and Improved Traceability.\"\n",
            "3. **Provide more concrete examples**: While the draft provides some examples of RAG applications, it would be beneficial to provide more concrete examples or case studies to illustrate the effectiveness of RAG in real-world scenarios.\n",
            "4. **Consider adding a section on future directions**: The draft concludes with a summary of RAG evaluation, but it would be beneficial to discuss potential future directions or areas of research that could further improve RAG. This could include advancements in vector databases, LLMs, or other related technologies.\n",
            "5. **Use a more formal tone in the TL;DR**: While the TL;DR is concise and effective, it could be written in a more formal tone to match the rest of the draft.\n",
            "\n",
            "Overall, the draft is well-written and effectively communicates the concept of RAG evaluation. With some minor revisions, it could be even more effective in engaging the reader and conveying the benefits of RAG.\n",
            "Approve and finalize? (y/n): y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Final Blog Output"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Title:** Unlocking the Power of Retrieval Augmented Generation (RAG) Evaluation\n================================================================\n\n**Meta:** Discover the benefits and key components of RAG evaluation, a cutting-edge approach to content generation that combines retrieval and generation.\n\n**Slug:** retrieval-augmented-generation-evaluation\n\n**Tags:** RAG, Retrieval Augmented Generation, Content Generation, Large Language Models, Vector Databases, Hallucinations, Traceability\n\n### Introduction to Retrieval Augmented Generation (RAG) Evaluation\n\nRetrieval Augmented Generation (RAG) is a powerful approach to content generation that combines the strengths of retrieval and generation. By leveraging a vector database and a Large Language Model (LLM), RAG enables the creation of high-quality content while reducing hallucinations and improving traceability. This evaluation framework is essential for developers and content creators who want to harness the full potential of RAG.\n\n### Key Components of RAG Evaluation\n\n#### 1. Pipeline Overview\n\nThe RAG pipeline involves several key components:\n\n* **Splitting Data**: Divide input data into manageable chunks for processing. This is crucial for efficient processing and reduces the computational overhead.\n* **Generating Embeddings**: Create vector representations of input data. These embeddings serve as a common language for the vector database and the LLM.\n* **Creating a Vector Database**: Store embeddings in a database for efficient retrieval. This allows for fast lookup and retrieval of relevant information.\n* **Retrieving Information**: Use the vector database to retrieve relevant information. This is where the retrieval component of RAG comes into play.\n* **Using a Large Language Model (LLM)**: Generate final content using the retrieved information and LLM. This is where the generation component of RAG comes into play.\n\n#### 2. Benefits of RAG Evaluation\n\nRAG evaluation offers several benefits, including:\n\n* **Reduced Hallucinations**: RAG reduces the likelihood of generated content being incorrect or misleading. This is because RAG relies on the retrieval of existing knowledge, rather than generating content from scratch.\n* **Access to Fresher Knowledge**: RAG enables the use of up-to-date information and knowledge. This is because the vector database can be updated in real-time, allowing for the incorporation of new information.\n* **Improved Traceability**: RAG provides a clear audit trail of the content generation process. This makes it easier to identify the sources of information and the reasoning behind the generated content.\n\n#### 3. Key Considerations for RAG Evaluation\n\nWhen implementing RAG evaluation, there are several key considerations to keep in mind:\n\n* **Choosing an Effective Chunking Strategy**: Divide input data into optimal chunks for processing. This can significantly impact the performance and efficiency of the RAG pipeline.\n* **Ensuring Prompt Grounding**: Ensure that the LLM is properly grounded in the input data. This is crucial for generating high-quality content that is relevant to the input data.\n* **Evaluating the Performance of the System**: Regularly assess the quality and accuracy of generated content. This can help identify areas for improvement and optimize the RAG pipeline.\n\n### Application of RAG Evaluation\n\nRAG evaluation has a wide range of applications, including:\n\n#### 1. Content Strategy\n\n* **Blog Workflow**: Use RAG to generate high-quality blog posts. This can help reduce the workload of content creators and improve the consistency of blog posts.\n* **Prompting**: Use RAG to generate effective prompts for LLMs. This can help improve the quality and relevance of generated content.\n* **Citations**: Use RAG to generate accurate citations and references. This can help improve the credibility and reliability of generated content.\n\n### Limitations and Future Directions\n\nWhile RAG evaluation offers numerous benefits, there are several limitations and challenges associated with its implementation. These include:\n\n* **Data Quality**: RAG relies on high-quality data to generate accurate and relevant content. Poor data quality can lead to hallucinations and decreased performance.\n* **Scalability**: RAG can be computationally intensive, particularly when dealing with large datasets. This can make it challenging to scale RAG to meet the needs of large organizations.\n* **Specialized Expertise**: RAG requires specialized expertise in areas such as natural language processing, vector databases, and LLMs. This can make it challenging to implement RAG without significant investment in training and resources.\n\n### Conclusion\n================================================================\n\nIn conclusion, RAG evaluation is a powerful approach to content generation that offers numerous benefits, including reduced hallucinations, access to fresher knowledge, and improved traceability. By understanding the key components of RAG evaluation, including the pipeline overview, benefits, and key considerations, content creators and developers can effectively leverage RAG to generate high-quality content.\n\n**TL;DR**: RAG evaluation is a powerful approach to content generation that combines the strengths of retrieval and generation. By leveraging a vector database and a Large Language Model (LLM), RAG enables the creation of high-quality content while reducing hallucinations and improving traceability.\n\n**Target Words: 500**\n\n**Word Count: 517**"
          },
          "metadata": {}
        }
      ]
    }
  ]
}